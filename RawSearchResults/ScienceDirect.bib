@article{BRITO2018306,
title = {On the use of replacement messages in API deprecation: An empirical study},
journal = {Journal of Systems and Software},
volume = {137},
pages = {306-321},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S016412121730300X},
author = {Gleison Brito and Andre Hora and Marco Tulio Valente and Romain Robbes},
keywords = {API deprecation, Software evolution, Mining software repositories, Empirical software engineering},
abstract = {Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages.}
}
@article{ESPINHA201527,
title = {Web API growing pains: Loosely coupled yet strongly tied},
journal = {Journal of Systems and Software},
volume = {100},
pages = {27-43},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002180},
author = {Tiago Espinha and Andy Zaidman and Hans-Gerhard Gross},
keywords = {Web API, Software evolution, Breaking changes},
abstract = {Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution.}
}
@article{NADAL20193,
title = {An integration-oriented ontology to govern evolution in Big Data ecosystems},
journal = {Information Systems},
volume = {79},
pages = {3-19},
year = {2019},
note = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917304660},
author = {Sergi Nadal and Oscar Romero and Alberto Abelló and Panos Vassiliadis and Stijn Vansummeren},
keywords = {Data integration, Evolution, Semantic web},
abstract = {Big Data architectures allow to flexibly store and process heterogeneous data, from multiple sources, in their original format. The structure of those data, commonly supplied by means of REST APIs, is continuously evolving. Thus data analysts need to adapt their analytical processes after each API release. This gets more challenging when performing an integrated or historical analysis. To cope with such complexity, in this paper, we present the Big Data Integration ontology, the core construct to govern the data integration process under schema evolution by systematically annotating it with information regarding the schema of the sources. We present a query rewriting algorithm that, using the annotated ontology, converts queries posed over the ontology to queries over the sources. To cope with syntactic evolution in the sources, we present an algorithm that semi-automatically adapts the ontology upon new releases. This guarantees ontology-mediated queries to correctly retrieve data from the most recent schema version as well as correctness in historical queries. A functional and performance evaluation on real-world APIs is performed to validate our approach.}
}
@article{AZPEITIA2020101468,
title = {Volunteering for Linked Data Wrapper maintenance: A platform perspective},
journal = {Information Systems},
volume = {89},
pages = {101468},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101468},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919305204},
author = {Iker Azpeitia and Jon Iturrioz and Oscar Díaz},
keywords = {Linked Data Wrappers, Maintenance, Open platforms, Volunteering, YQL},
abstract = {Linked Data Wrappers (LDWs) turn Web APIs into RDF end-points, leveraging the Linked Open Data cloud with current data. Unfortunately, LDWs are fragile upon upgrades on the underlying APIs, compromising LDW stability. Hence, for API-based LDWs to become a sustainable foundation for the Web of Data, we should recognize LDW maintenance as a continuous effort that outlives their breakout projects. This is not new in Software Engineering. Other projects in the past faced similar issues. The strategy: becoming open source and turning towards dedicated platforms. By making LDWs open, we permit others not only to inspect (hence, increasing trust and consumption), but also to maintain (to cope with API upgrades) and reuse (to adapt for their own purposes). Promoting consumption, adaptation and reuse might all help to increase the user base, and in so doing, might provide the critical mass of volunteers, current LDW projects lack. Drawing upon the Helping Theory, we investigate three enablers of volunteering applied to LDW maintenance: impetus to respond, positive evaluation of contributing and increasing awareness. Insights are fleshed out through SYQL, a LDW platform on top of Yahoo’s YQL. Specifically, SYQL capitalizes on the YQL community (i.e. impetus to respond), providesannotation overlays to easy participation (i.e. positive evaluation of contributing), and introduces aHealth Checker (i.e. increasing awareness). Evaluation is conducted for 12 subjects involved in maintaining someone else’s LDWs. Results indicate that both the Health Checker and the annotation overlays provide utility as enablers of awareness and contribution.}
}
@article{HIGO2020110571,
title = {On tracking Java methods with Git mechanisms},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110571},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110571},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300522},
author = {Yoshiki Higo and Shinpei Hayashi and Shinji Kusumoto},
keywords = {Mining software repositories, Source code analysis, Tracking Java methods},
abstract = {Method-level historical information is useful in various research on mining software repositories such as fault-prone module detection or evolutionary coupling identification. An existing technique named Historage converts a Git repository of a Java project to a finer-grained one. In a finer-grained repository, each Java method exists as a single file. Treating Java methods as files has an advantage, which is that Java methods can be tracked with Git mechanisms. The biggest benefit of tracking methods with Git mechanisms is that it can easily connect with any other tools and techniques build on Git infrastructure. However, Historage’s tracking has an issue of accuracy, especially on small methods. More concretely, in the case that a small method is renamed or moved to another class, Historage has a limited capability to track the method. In this paper, we propose a new technique, FinerGit, to improve the trackability of Java methods with Git mechanisms. We implement FinerGit as a system and apply it to 182 open source software projects, which include 1,768K methods in total. The experimental results show that our tool has a higher capability of tracking methods in the case that methods are renamed or moved to other classes.}
}
@article{RUHROTH2012270,
title = {Model evolution and refinement},
journal = {Science of Computer Programming},
volume = {77},
number = {3},
pages = {270-289},
year = {2012},
note = {Feature-Oriented Software Development (FOSD 2009)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2011.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167642311001298},
author = {Thomas Ruhroth and Heike Wehrheim},
keywords = {Evolution, Refinement, Formal methods, Object-Z, Refactoring},
abstract = {Software changes during its lifetime. Likewise, software models change during their design time, e.g. by removing, adding or changing operations and classes. This is referred to as model evolution. In a refinement-based approach to software design, we moreover do not deal with a single but with a chain of models (viz. formal specifications), related via refinement. Changes thus need to be consistently made to all specifications in the chain so as to keep the refinement structure. In this paper, we develop co-evolutions of models in the context of the formal method Object-Z. More specifically, given a particular evolution of a specification we show how to construct a corresponding evolution for its refinements such that the refinement relationship is kept. A chain of models can thus be systematically and consistently evolved, while maintaining the given refinement structure.}
}
@article{WILLIS1992169,
title = {Phylogenetic relationships in the honeybee (Genus Apis) as determined by the sequence of the cytochrome oxidase II region of mitochondrial DNA},
journal = {Molecular Phylogenetics and Evolution},
volume = {1},
number = {3},
pages = {169-178},
year = {1992},
issn = {1055-7903},
doi = {https://doi.org/10.1016/1055-7903(92)90013-7},
url = {https://www.sciencedirect.com/science/article/pii/1055790392900137},
author = {Leslie G. Willis and Mark L. Winston and Barry M. Honda},
abstract = {The complete nucleotide sequence of the mitochondrial cytochrome oxidase II (COH) gene was determined for five species of the honeybee (Genus: Apis): A. andreniformis, A. cerana, A. dorsata, A. fiorea, and A. koschevnikovi, these were then compared to the known sequence of the A. millifera gene fromCrozier et al. (1989, Mol. Biol. Evol., 6: 399–411) and the wasp Excristes roborator (Liu and Beckenbach, 1992, Mol. Phylogenet. Evol., 1:41–52). Phylogenetic relationships were derived using the parsimony methods DNAPARS and PROTPARS of Felsenstein (“PHYLIP Manual Version 3.4, ”University Herbarium, Univ. of California, Berkeley). The results suggest that A. dorsata is the most ancestral species, followed by the branching of A. florea/A. andreniformis and A. koschevnikovi, and then A. mellifera and A. cerana. This inference differs from the currently accepted view that considers the A. florea/A. andreniformis line to be the most ancestral.}
}
@article{BINKLEY201595,
title = {Editorial of special section from Software Evolution Week 2014},
journal = {Information and Software Technology},
volume = {65},
pages = {95-96},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000567},
author = {David Binkley and Filippo Ricca and Demeyer Serge}
}
@article{PADMANABAN2018,
title = {Computability evaluation of RESTful API using Primitive Recursive Function},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818309157},
author = {R. Padmanaban and M. Thirumaran and P. Anitha and A. Moshika},
keywords = {RESTful API, Distributed environment, Primitive Recursive Function (PRF), Turing machine, Primitive Recursive Resources (PRR), Application logic, AppState logic},
abstract = {Web services are moving toward a new emerging technology lead to the migration of SOAP to RESTful API, which is an Architectural Style that holds Lightweight, Stateless, Uniform Interface, etc., as its constraints. Various sources of clusters of resources, entity, database relations are access throughout the distributed environment across the internet. Generative Power of the RESTful API witnesses the emergence of many companies whose whole business process is based upon the building applications. Since the Syntactic essentials of RESTful Web Services are mainly concerned with the RESTful API, there is a need for evaluation of those essentials whether they are computable or not. The proposed work is carried out on taking resources as simple and effective using Primitive Recursive Function (PRF). Primitive Recursive Function (PRF) uses Turing Machine for REST API capability evaluation and the Service Invocation along with the Application Logic and AppState Logic in order to handle manageability of the RESTful resources via computability evaluation with or without security. To demonstrate the effectiveness of our evaluation process, we conduct a case study on the available REST web services using Primitive Recursive Resources (PRR). The results of our case study show that our evaluation process achieves greater portability, reliability, scalability, etc., which in turn results in high performance.}
}
@article{JEZEK2015129,
title = {How Java APIs break – An empirical study},
journal = {Information and Software Technology},
volume = {65},
pages = {129-146},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000506},
author = {Kamil Jezek and Jens Dietrich and Premek Brada},
keywords = {Binary compatibility, API evolution, Backward compatibility, Byte-code, Java},
abstract = {Context
It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.
Objective
We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries.
Method
This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library.
Results
We found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API.
Conclusion
It is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution.}
}
@article{KULA2018186,
title = {An empirical study on the impact of refactoring activities on evolving client-used APIs},
journal = {Information and Software Technology},
volume = {93},
pages = {186-199},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304780},
author = {Raula Gaikovina Kula and Ali Ouni and Daniel M. German and Katsuro Inoue},
keywords = {Refactoring, API Breakages, Software libraries, Software evolution},
abstract = {Context
Refactoring is recognized as an effective practice to maintain evolving software systems. For software libraries, we study how library developers refactor their Application Programming Interfaces (APIs), especially when it impacts client users by breaking an API of the library.
Objective
Our work aims to understand how clients that use a library API are affected by refactoring activities. We target popular libraries that potentially impact more library client users.
Method
We distinguish between library APIs based on their client-usage (referred to as client-used APIs) in order to understand the extent to which API breakages relate to refactorings. Our tool-based approach allows for a large-scale study across eight libraries (i.e., totaling 183 consecutive versions) with around 900 clients projects.
Results
We find that library maintainers are less likely to break client-used API classes. Quantitatively, we find that refactoring activities break less than 37% of all client-used APIs. In a more qualitative analysis, we show two documented cases of where non-refactoring API breaking changes are motivated other maintenance issues (i.e., bug fix and new features) and involve more complex refactoring operations.
Conclusion
Using our automated approach, we find that library developers are less likely to break APIs and tend to break client-used APIs when performing maintenance issues.}
}
@article{JHA2019164,
title = {An empirical study of configuration changes and adoption in Android apps},
journal = {Journal of Systems and Software},
volume = {156},
pages = {164-180},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.06.095},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301396},
author = {Ajay Kumar Jha and Sunghee Lee and Woo Jin Lee},
keywords = {Android app evolution, App maintenance, Configuration changes, Effort estimation, Permission evolution},
abstract = {Android platform is evolving rapidly. Therefore, evolution and maintenance of Android apps are major concerns among developers. One of the essential components of each app is an Android manifest file, which is a configuration file used to declare various key attributes of apps. This paper presents an empirical study to understand app evolution through configuration changes. The results of this study will help developers in identifying change-proneness attributes, including change patterns and the reason behind the change, understanding the adoption of different attributes introduced in different versions of the Android platform, and understanding effort distribution pattern in configuration changes and taking proactive measures to reduce the effort. In this paper, we use a data mining approach. We analyze commit histories of Android manifest files of 908 apps to understand the app evolution. The results of this study show that most of the apps extend core functionalities and improve user interface over time, configuration changes are mostly influenced by functionalities extension, platform evolution, and bug reports, very few numbers of existing apps adopt new attributes introduced by the platform, apps are generally slow in adopting new attributes, and significant effort is wasted in changing configuration and then reverting back the change.}
}
@article{MUKHIYA2019338,
title = {A GraphQL approach to Healthcare Information Exchange with HL7 FHIR},
journal = {Procedia Computer Science},
volume = {160},
pages = {338-345},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.082},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931782X},
author = {Suresh Kumar Mukhiya and Fazle Rabbi and Violet Ka {I Pun} and Adrian Rutle and Yngve Lamo},
keywords = {GraphQL, HL7 FHIR, REST API, overfetching, underfetching, Health Information Exchange, Interoperability, REST vs GraphQL},
abstract = {Interoperability is accepted as a fundamental necessity for the successful realization of Healthcare Information Systems. It can be achieved by utilizing consistent standards defining syntactic and semantic meaning of the information being exchanged. HL7 FHIR is one of such open standards for Health Information Exchange (HIE). While HL7 FHIR supports Representational State Transfer (REST) architecture and Service-oriented Architecture (SOA) for seamless information exchange, it inherits the inflexibility and complexity associated with the RESTful approach. GraphQL is a query language developed by Facebook that provides promising techniques to overcome these issues. In this paper, we exploit the use of GraphQL and HL7 FHIR for HIE; present an algorithm to map HL7 FHIR resources to a GraphQL schema, and created a prototype implementation of the approach and compare it with a RESTful approach. Our experimental results indicate that the combination of GraphQL and HL7 FHIR-based web APIs for HIE is performant, cost-effective, scalable and flexible to meet web and mobile clients requirements.}
}
@article{HORA2015192,
title = {Automatic detection of system-specific conventions unknown to developers},
journal = {Journal of Systems and Software},
volume = {109},
pages = {192-204},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001727},
author = {André Hora and Nicolas Anquetil and Anne Etien and Stéphane Ducasse and Marco Túlio Valente},
keywords = {Automatic coding convention detection, Mining software repositories, Software evolution},
abstract = {In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java’s method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts.}
}
@article{MOSQUEIRAREY201846,
title = {A systematic approach to API usability: Taxonomy-derived criteria and a case study},
journal = {Information and Software Technology},
volume = {97},
pages = {46-63},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302471},
author = {Eduardo Mosqueira-Rey and David Alonso-Ríos and Vicente Moret-Bonillo and Isaac Fernández-Varela and Diego Álvarez-Estévez},
keywords = {Usability, Usability taxonomies, Usability studies, Application program interfaces, APIs, Sleep medicine},
abstract = {Context
The currently existing literature about Application Program Interface (API) usability is heterogeneous in terms of goals, scope, and audience; and its connection to accepted definitions of usability is rarely made explicit. The use of metrics to measure API usability is focused only on measurable characteristics excluding those usability aspects that are related to the subjectivity of human opinions.
Objective
Our objective is to build a comprehensive set of heuristics and guidelines for API usability that is a structured synthesis of the existing literature on API usability but which also covers other aspects that have been neglected so far. This set is explicitly connected with a usability model, something that allows us to check if we are addressing actual usability problems.
Method
Our approach is to follow a systematic approach based on a comprehensive model of usability and context-of-use. From this comprehensive model we derived the set of heuristics and guidelines that are used to carry out a heuristic evaluation with usability experts and a subjective analysis with users. The influence of the context of use, something that is normally ignored, is explicitly analyzed.
Results
Our heuristics and guidelines were integrated into a usability study of a sleep medicine API. In this study, we were able to identify several usability issues of the proposed API that are not explicitly addressed in the existing literature. The context of use helped us to identify those categories that were more relevant to consider in order to improve API usability.
Conclusion
The literature on API usability is very technically-minded and tends to neglect the subjective component of usability. We contribute to a more global and comprehensive view of the usability of APIs that is not contradictory but complementary with metrics. Our criteria ease the always necessary usability evaluation with human evaluators and users.}
}
@article{KARBAB2018S48,
title = {MalDozer: Automatic framework for android malware detection using deep learning},
journal = {Digital Investigation},
volume = {24},
pages = {S48-S59},
year = {2018},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618300392},
author = {ElMouatez Billah Karbab and Mourad Debbabi and Abdelouahid Derhab and Djedjiga Mouheb},
keywords = {Mobile, Android, Malware, IoT, Deep learning},
abstract = {Android OS experiences a blazing popularity since the last few years. This predominant platform has established itself not only in the mobile world but also in the Internet of Things (IoT) devices. This popularity, however, comes at the expense of security, as it has become a tempting target of malicious apps. Hence, there is an increasing need for sophisticated, automatic, and portable malware detection solutions. In this paper, we propose MalDozer, an automatic Android malware detection and family attribution framework that relies on sequences classification using deep learning techniques. Starting from the raw sequence of the app's API method calls, MalDozer automatically extracts and learns the malicious and the benign patterns from the actual samples to detect Android malware. MalDozer can serve as a ubiquitous malware detection system that is not only deployed on servers, but also on mobile and even IoT devices. We evaluate MalDozer on multiple Android malware datasets ranging from 1 K to 33 K malware apps, and 38 K benign apps. The results show that MalDozer can correctly detect malware and attribute them to their actual families with an F1-Score of 96%–99% and a false positive rate of 0.06%–2%, under all tested datasets and settings.}
}
@article{MANIKAS201684,
title = {Revisiting software ecosystems Research: A longitudinal literature study},
journal = {Journal of Systems and Software},
volume = {117},
pages = {84-103},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216000406},
author = {Konstantinos Manikas},
keywords = {Software ecosystems, Longitudinal literature study, Software ecosystem maturity},
abstract = {‘Software ecosystems’ is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems.}
}
@article{PINTO201559,
title = {A large-scale study on the usage of Java’s concurrent programming constructs},
journal = {Journal of Systems and Software},
volume = {106},
pages = {59-81},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.04.064},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215000849},
author = {Gustavo Pinto and Weslley Torres and Benito Fernandes and Fernando Castor and Roberto S.M. Barros},
keywords = {Java, Concurrency, Software evolution},
abstract = {In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.}
}
@article{SHEN2016171,
title = {An analysis of multifractal characteristics of API time series in Nanjing, China},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {451},
pages = {171-179},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2016.01.061},
url = {https://www.sciencedirect.com/science/article/pii/S0378437116001084},
author = {Chen-hua Shen and Yi Huang and Ya-ni Yan},
keywords = {Multifractality, Multifractal detrended fluctuation analysis, Multifractal characteristics, Long-range correlations, A broad probability density function, API},
abstract = {This paper describes multifractal characteristics of daily air pollution index (API) records in Nanjing from 2001 to 2012. The entire daily API time series is first divided into 12 parts that serve as research objects, and the generalized Hurst exponent is calculated for each series. And then, the multifractal sources are analyzed and singularity spectra are shown. Next, based on a singularity spectrum, the multifractal-characteristics parameters (maximum exponent α0, spectrum width Δα, and asymmetry Δαas) are introduced. The results show that the fractality of daily API for each year is multifractal. The multifractal sources originate from both a broad probability density function and different long-range correlations with small and large fluctuations. The strength of the distribution multifractality is stronger than that of the correlation multifractality. The variation in the structure of API time series with increasing years is mainly related to long-range correlations. The structure of API time series in some years is richer. These findings can provide a scientific basis for further probing into the complexity of API.}
}
@incollection{GORANSSON2017353,
title = {Chapter 15 - SDN Futures},
editor = {Paul Göransson and Chuck Black and Timothy Culver},
booktitle = {Software Defined Networks (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {353-374},
year = {2017},
isbn = {978-0-12-804555-8},
doi = {https://doi.org/10.1016/B978-0-12-804555-8.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045558000156},
author = {Paul Göransson and Chuck Black and Timothy Culver},
keywords = {Optical offload, Mobile traffic offload, Wireless backhaul, Radio Access Network, Network debugger, Media-independent handovers, Energy savings, SD-WAN},
abstract = {This chapter begins with a review of the current state of affairs of SDN, mapping SDN’s history and projected future to the well-known Gartner Hype Cycle. While it is easy to conclude that the turbulence and some of the pessimism surrounding SDN in recent times reflected a trough of disillusionment for SDN, it is also easy to remain optimistic about its future. It will surely take years to determine just what forms of SDN turn out to be dominant in the data centers of the future. This chapter argues that the optimism for SDN’s future derives not only from its impact on its most often-cited area of application, the data center, but also for the many areas of networking where we have only begun to consider the SDN toolkit. In this chapter we consider a number of such new potential applications. We begin this discussion with how SD-WAN works and consider the reasons for its rapid growth. We continue with a number of new applications ideas that are specifically tailored for Open SDN. This includes managing nontraditional physical layers’ links, such as optical networks. We consider new security applications for SDN. SDN can also play a role in hand-offs between different kinds of mobile networks. This chapter also covers new applications for traffic engineering in mobile networks as well for energy savings in networking equipment. We also discuss some ideas for SDN-enabled switching chips that bring an SDN-enabled future closer to our grasp. We conclude the chapter and book with a brief assessment of what the future looks like not only for Open SDN but for the major alternative SDN technologies presented earlier in this work.}
}
@article{MESFIN2016205,
title = {Towards end-user development of REST client applications on smartphones},
journal = {Computer Standards & Interfaces},
volume = {44},
pages = {205-219},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2015.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548915000896},
author = {Gebremariam Mesfin and Tor-Morten Grønli and Dida Midekso and Gheorghita Ghinea},
keywords = {REST service, Usability, Service composition, Smartphone, Cross-platform},
abstract = {HTML5 can be used to develop client applications by composing REST web services within the context of Web 2.0. However, the possibility of implementing cross-platform smartphone applications with REST services needs to be studied. Accordingly, we developed a REST-based cross-platform application with PhoneGap. The application was deployed on the Android, Windows Phone, and iOS platforms; subsequently we evaluated its usability. We observed that REST-based cross-platform smartphone applications can be implemented with HTML5 and PhoneGap, which can be scaled-up into a REST service composition tool. Moreover, the application’s usability remains unaffected on the native platforms and adaptation required only minimal effort.}
}
@incollection{GORANSSON2014281,
title = {Chapter 13 - SDN Futures},
editor = {Paul Goransson and Chuck Black},
booktitle = {Software Defined Networks},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {281-294},
year = {2014},
isbn = {978-0-12-416675-2},
doi = {https://doi.org/10.1016/B978-0-12-416675-2.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166752000139},
author = {Paul Goransson and Chuck Black},
keywords = {Gartner hype cycle, Big pivot, Optical offload, Mobile traffic offload, Wireless backhaul, Radio access network, Network simulation, Network debugger, Media-independent handovers, Point of access, Point of service, OmniRAN, Energy savings, ElasticTree, SDN-specific ASICs},
abstract = {Chapter 13 begins with a review of the current state of affairs of SDN, mapping SDN’s history and projected future to the well-known Gartner Hype Cycle. While it is easy to conclude that the turbulence and some of the pessimism surrounding SDN in 2013 reflected a trough of disillusionment for SDN, it is also easy to remain optimistic about its future. It will surely take years to determine just what forms of SDN turn out to be dominant in the data centers of the future. Chapter 13 argues that the optimism for SDN’s future derives not only from its impact on its most often-cited area of application, the data center, but also for the many areas of networking where we have only begun to consider the SDN toolkit. In this chapter we consider a number of such new potential applications. This includes managing non-traditional physical layers’ links, such as optical networks. We consider new security applications for SDN. SDN can also play a role in hand-offs between different kinds of mobile networks. This chapter also covers new applications for traffic engineering in mobile networks as well for energy savings in networking equipment. We also discuss some ideas for SDN-enabled switching chips that bring an SDN-enabled future closer to our grasp. The new applications discussed in this chapter are specifically tailored for Open SDN. We conclude the chapter and book with a brief assessment of what the future looks like not only for Open SDN but for the major alternative SDN technologies presented earlier in this work.}
}